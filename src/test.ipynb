{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from accelerate import Accelerator, FullyShardedDataParallelPlugin, init_empty_weights, infer_auto_device_map\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig,\n",
    "                          TrainingArguments, Trainer,\n",
    "                          DataCollatorForLanguageModeling)\n",
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from configparser import ConfigParser\n",
    "from utils import generate_and_tokenize_prompt, generate_and_tokenize_prompt2, plot_data_lengths, print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "base_path = \"/data/arguellesa/traice/\"\n",
    "data_train_path = 'pcapnew.json'\n",
    "data_test_path = 'pcapeval.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "c = ConfigParser()\n",
    "c.read(\"model/.config\")\n",
    "hf_token = c.get(\"DEFAULT\", \"token\")\n",
    "base_model_id = c.get(\"DEFAULT\", \"model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_dataset = load_dataset('json', data_files=base_path+data_train_path, split='train')\n",
    "eval_dataset = load_dataset('json', data_files=base_path+data_test_path, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, padding_side=\"left\", add_eos_token=True, add_bos_token=True,\n",
    "                                          token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_train_dataset = train_dataset.map(lambda x : generate_and_tokenize_prompt(tokenizer, x))\n",
    "tokenized_val_dataset = eval_dataset.map(lambda x : generate_and_tokenize_prompt(tokenizer, x))\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1700\n",
    "tokenized_train_dataset = train_dataset.map(lambda x : generate_and_tokenize_prompt2(tokenizer, x, max_length=max_length))\n",
    "tokenized_val_dataset = eval_dataset.map(lambda x : generate_and_tokenize_prompt2(tokenizer, x, max_length=max_length))\n",
    "\n",
    "print(tokenized_train_dataset[0]['input_ids'])\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model before fine-tuning\n",
    "eval_prompt = \"\"\"\\n\\nNo.\\tTime\\tSource\\tDestination\\tProtocol\\tLength\\tInfo\\n\n",
    "1\\t0\\t192.168.0.2\\t142.251.32.174\\tTCP\\t74\\t37436  >  443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 TSval=1393734230 TSecr=0 WS=128\n",
    "\\n2\\t0.040585\\t142.251.32.174\\t192.168.0.2\\tTCP\\t58\\t443  >  37436 [SYN, ACK] Seq=0 Ack=1 Win=65535 Len=0 MSS=1460\n",
    "\\n3\\t0.042195\\t192.168.0.2\\t142.251.32.174\\tTCP\\t60\\t37436  >  443 [ACK] Seq=1 Ack=1 Win=64240 Len=0\n",
    "\\n4\\t0.048234\\t192.168.0.2\\t142.251.32.174\\tTLSv1.3\\t444\\tClient Hello\n",
    "\\n5\\t0.048712\\t142.251.32.174\\t192.168.0.2\\tTCP\\t54\\t443  >  37436 [ACK] Seq=1 Ack=391 Win=65535 Len=0\n",
    "\\n6\\t0.090352\\t142.251.32.174\\t192.168.0.2\\tTLSv1.3\\t1484\\tServer Hello, Change Cipher Spec\n",
    "\\n7\\t0.091339\\t192.168.0.2\\t142.251.32.174\\tTCP\\t60\\t37436  >  443 [ACK] Seq=391 Ack=1431 Win=62920 Len=0\n",
    "\\n8\\t0.09594\\t142.251.32.174\\t192.168.0.2\\tTCP\\t4434\\t443  >  37436 [ACK] Seq=1431 Ack=391 Win=65535 Len=4380 [TCP segment of a reassembled PDU]\n",
    "\\n9\\t0.096204\\t192.168.0.2\\t142.251.32.174\\tTCP\\t60\\t37436  >  443 [ACK] Seq=391 Ack=5811 Win=61320 Len=0\n",
    "\\n10\\t0.096295\\t142.251.32.174\\t192.168.0.2\\tTLSv1.3\\t884\\tApplication Data\n",
    "\\n11\\t0.096504\\t192.168.0.2\\t142.251.32.174\\tTCP\\t60\\t37436  >  443 [ACK] Seq=391 Ack=6641 Win=62780 Len=0\n",
    "\\n12\\t0.10038\\t192.168.0.2\\t142.251.32.174\\tTLSv1.3\\t134\\tChange Cipher Spec, Application Data\n",
    "\\n13\\t0.100832\\t142.251.32.174\\t192.168.0.2\\tTCP\\t54\\t443  >  37436 [ACK] Seq=6641 Ack=471 Win=65535 Len=0\n",
    "\\n14\\t0.101101\\t192.168.0.2\\t142.251.32.174\\tTLSv1.3\\t213\\tApplication Data\n",
    "\\n15\\t0.101512\\t142.251.32.174\\t192.168.0.2\\tTCP\\t54\\t443  >  37436 [ACK] Seq=6641 Ack=630 Win=65535 Len=0\n",
    "\\n16\\t0.150147\\t142.251.32.174\\t192.168.0.2\\tTLSv1.3\\t1345\\tApplication Data, Application Data\n",
    "\\n17\\t0.150373\\t192.168.0.2\\t142.251.32.174\\tTCP\\t60\\t37436  >  443 [ACK] Seq=630 Ack=7932 Win=62780 Len=0\n",
    "\\n18\\t0.199491\\t192.168.0.2\\t142.251.45.36\\tTCP\\t74\\t48312  >  443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 TSv\n",
    "\\n\\nExplain the table, covering login attempts, brute force attacks, certificate issues, and other network activities. Include causes, detection methods, and mitigation strategies for each scenario. Aim for clarity and specificity, suitable for both technical and non-technical audiences.\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=max_length, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up lora\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(model)\n",
    "\n",
    "# LoRA configuration and model preparation\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the accelerator\n",
    "# you can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases setup\n",
    "wandb.login()\n",
    "wandb_project = \"traice\"\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configuration\n",
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "project = \"traice\"\n",
    "base_model_name = \"llama2-13b\"\n",
    "run_name = f\"{base_model_name}-{project}\"\n",
    "output_dir = f\"{base_path}{run_name}\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training execution\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(base_model_id)\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModelForCausalLM.from_config(config)\n",
    "# model.tie_weights()\n",
    "\n",
    "# device_map = infer_auto_device_map(model, max_memory={0: \"30GiB\", 1: \"30GiB\"},\n",
    "#                                    no_split_module_classes=['InstructBlipEncoderLayer', 'InstructBlipQFormerLayer',\n",
    "#                                                             'LlamaDecoderLayer'])\n",
    "\n",
    "# offload = \"offload\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_id, token=hf_token, device_map=device_map,\n",
    "#                                              offload_folder=offload, offload_state_dict=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
